{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c333cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.2-py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 16.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel in c:\\users\\younjeongoh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\younjeongoh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from lightgbm) (1.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\younjeongoh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from lightgbm) (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\younjeongoh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from lightgbm) (1.22.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\younjeongoh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\younjeongoh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e69ef4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f733d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# batch_size = 32\n",
    "# drop_rate = 0.3\n",
    "\n",
    "max_depth = 5\n",
    "n_estimators = 5000             # 400\n",
    "lr = 0.01\n",
    "num_leaves = 31\n",
    "early_stopping_rounds = 300      # 100\n",
    "verbose = 500                  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05f64fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\weather\\\\v_lgbm'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_dir = os.getcwd()\n",
    "curr_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "864c45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(curr_dir):\n",
    " \n",
    "    path_train = os.path.join(curr_dir+\"\\\\data_all\\\\\"+\"\\\\train\")\n",
    "    path_test = os.path.join(curr_dir+\"\\\\data_all\\\\\"+\"\\\\test\")\n",
    "    \n",
    "    folders_tr = os.listdir(path_train)\n",
    "    folders_test = os.listdir(path_test)\n",
    "    \n",
    "    train = pd.DataFrame()\n",
    "    for files in folders_tr:\n",
    "        df= pd.read_csv(os.path.join(curr_dir+\"\\\\data_all\\\\\"+\"\\\\train\\\\\")+files)\n",
    "        train = pd.concat([train, df])            \n",
    "              \n",
    "    \n",
    "    test = pd.DataFrame()\n",
    "    for files in folders_test:\n",
    "        df= pd.read_csv(os.path.join(curr_dir+\"\\\\data_all\\\\\"+\"\\\\test\\\\\")+files)\n",
    "        test = pd.concat([test, df])   \n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "    X_train = train[['Lat','SolarZA','SateZA','ESR','Band1','Band2','Band3','Band4','Band6','Band7', '30daysBand3','30daysBand13',\\\n",
    "                     'GK2A-LST','insitu-TD','insitu-TG', 'insitu-HM','insitu-PS','insitu-TED0.05','insitu-TED0.1','insitu-TED0.2',\\\n",
    "                     'insitu-TED0.3','insitu-TED0.5','insitu-TED1.0','insitu-TED1.5','insitu-TED5.0']]\n",
    "    X_test = test[['Lat','SolarZA','SateZA','ESR','Band1','Band2','Band3','Band4','Band6','Band7', '30daysBand3','30daysBand13',\\\n",
    "                     'GK2A-LST','insitu-TD','insitu-TG', 'insitu-HM','insitu-PS','insitu-TED0.05','insitu-TED0.1','insitu-TED0.2',\\\n",
    "                     'insitu-TED0.3','insitu-TED0.5','insitu-TED1.0','insitu-TED1.5','insitu-TED5.0']]\n",
    "    y_train_LST=train[['isitu-LST']].values\n",
    "    y_test_LST=test[['isitu-LST']].values\n",
    "    \n",
    "    y_train_TA=train[['insitu-TA']].values\n",
    "    y_test_TA=test[['insitu-TA']].values\n",
    "    \n",
    "#     model_name = '\\\\save\\\\'+'best_model_v1.h5' \n",
    "\n",
    "#     filepath = os.path.join(os.getcwd()+ model_name)\n",
    "\n",
    "#     es = EarlyStopping(monitor='val_mse', verbose=1, patience=20)\n",
    "\n",
    "#     checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "#                              monitor='val_mse',\n",
    "#                              verbose=1,\n",
    "#                              save_best_only=True)\n",
    "\n",
    "#     callbacks = [checkpoint, es]\n",
    "\n",
    "    lgbm_reg_LST = LGBMRegressor(objective= \"regression\",\n",
    "                          max_depth= max_depth,\n",
    "                          n_estimators= n_estimators,\n",
    "                          learning_rate= lr,\n",
    "                          num_leaves = num_leaves)\n",
    "    \n",
    "    lgbm_reg_LST.fit(X_train, y_train_LST,\n",
    "              eval_set=[(X_test, y_test_LST)],\n",
    "              eval_metric=[\"mse\"],\n",
    "              early_stopping_rounds=early_stopping_rounds,\n",
    "              verbose=verbose)\n",
    "    \n",
    "    lgbm_reg_TA = LGBMRegressor(objective= \"regression\",\n",
    "                          max_depth= max_depth,\n",
    "                          n_estimators= n_estimators,\n",
    "                          learning_rate= lr,\n",
    "                          num_leaves = num_leaves)\n",
    "    \n",
    "    lgbm_reg_TA.fit(X_train, y_train_TA,\n",
    "              eval_set=[(X_test, y_test_TA)],\n",
    "              eval_metric=[\"mse\"],\n",
    "              early_stopping_rounds=early_stopping_rounds,\n",
    "              verbose=verbose) \n",
    "    \n",
    " \n",
    "    \n",
    "#     lgbm_reg.load_weights(model_name)\n",
    "    \n",
    "    \n",
    "    path_val = os.path.join(curr_dir+\"\\\\data_all\\\\\"+\"\\\\validation\")\n",
    "    folders_val = os.listdir(path_val)\n",
    "    \n",
    "    validation = pd.DataFrame()\n",
    "    for files in folders_val:\n",
    "        df= pd.read_csv(os.path.join(curr_dir+\"\\\\data_all\\\\\"+\"\\\\validation\\\\\")+files)\n",
    "        validation = pd.concat([validation, df])        \n",
    "        \n",
    "    validation_ = validation[['Lat','SolarZA','SateZA','ESR','Band1','Band2','Band3','Band4','Band6','Band7', '30daysBand3','30daysBand13',\\\n",
    "                     'GK2A-LST','insitu-TD','insitu-TG', 'insitu-HM','insitu-PS','insitu-TED0.05','insitu-TED0.1','insitu-TED0.2',\\\n",
    "                     'insitu-TED0.3','insitu-TED0.5','insitu-TED1.0','insitu-TED1.5','insitu-TED5.0']]\n",
    "    prediction_LST = lgbm_reg_LST.predict(validation_)\n",
    "    prediction_TA = lgbm_reg_TA.predict(validation_)\n",
    "    \n",
    "    validation_partial = validation[['YearMonthDayHourMinute', 'STN']]\n",
    "    validation_partial = validation_partial.assign(LST = prediction_LST)\n",
    "    validation_partial = validation_partial.assign(TA = prediction_TA)\n",
    "    \n",
    "    submission=pd.read_csv('1-2_검증데이터셋.csv', encoding='utf-8')\n",
    "    submission1=submission.copy()\n",
    "    submission1[['isitu-LST', 'insitu-TA']] = validation_partial[['LST','TA']].values\n",
    "    \n",
    "    submission1.to_csv(\"lgbm_v1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62f3bac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\younjeongOh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\younjeongOh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\younjeongOh\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\lightgbm\\basic.py:179: UserWarning: Converting column-vector to 1d array\n",
      "  _log_warning('Converting column-vector to 1d array')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's l2: 4.61078\n",
      "[1000]\tvalid_0's l2: 4.24902\n",
      "[1500]\tvalid_0's l2: 4.05727\n",
      "[2000]\tvalid_0's l2: 3.9195\n",
      "[2500]\tvalid_0's l2: 3.81373\n",
      "[3000]\tvalid_0's l2: 3.73384\n",
      "[3500]\tvalid_0's l2: 3.67136\n",
      "[4000]\tvalid_0's l2: 3.62823\n",
      "[4500]\tvalid_0's l2: 3.59384\n",
      "[5000]\tvalid_0's l2: 3.56517\n",
      "[500]\tvalid_0's l2: 0.429099\n",
      "[1000]\tvalid_0's l2: 0.246352\n",
      "[1500]\tvalid_0's l2: 0.167129\n",
      "[2000]\tvalid_0's l2: 0.123268\n",
      "[2500]\tvalid_0's l2: 0.0982601\n",
      "[3000]\tvalid_0's l2: 0.0834404\n",
      "[3500]\tvalid_0's l2: 0.0730523\n",
      "[4000]\tvalid_0's l2: 0.0658277\n",
      "[4500]\tvalid_0's l2: 0.0600956\n",
      "[5000]\tvalid_0's l2: 0.0555484\n"
     ]
    }
   ],
   "source": [
    "model_train(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ff58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
